{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d1581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\A2_AIT\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:283: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cb64d",
   "metadata": {},
   "source": [
    "Task 1. Dataset Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47477f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: 3\n",
      "- 01 Harry Potter and the Sorcerers Stone.txt\n",
      "- 02 Harry Potter and the Chamber of Secrets.txt\n",
      "- 03 Harry Potter and the Prisoner of Azkaban.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path(\"data_raw/harry_potter_books\")\n",
    "assert RAW_DIR.exists(), f\"Folder not found: {RAW_DIR}\"\n",
    "\n",
    "txt_files = sorted(RAW_DIR.glob(\"*.txt\"))\n",
    "print(\"Found files:\", len(txt_files))\n",
    "for p in txt_files:\n",
    "    print(\"-\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59614afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded books: 3\n",
      "Example preview:\n",
      " M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amoun\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_book_text(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # collapse excessive blank lines\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    # remove trailing spaces on lines\n",
    "    text = \"\\n\".join([line.rstrip() for line in text.split(\"\\n\")])\n",
    "    return text.strip()\n",
    "\n",
    "books = []\n",
    "for fp in txt_files:\n",
    "    t = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    t = clean_book_text(t)\n",
    "    books.append(t)\n",
    "\n",
    "print(\"Loaded books:\", len(books))\n",
    "print(\"Example preview:\\n\", books[0][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f2fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\hp_kaggle_corpus.txt\n",
      "Chars: 1556494\n",
      "Tokens (whitespace): 274252\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "corpus_text = \"\\n\\n\".join(books)\n",
    "CORPUS_PATH = DATA_DIR / \"hp_kaggle_corpus.txt\"\n",
    "CORPUS_PATH.write_text(corpus_text, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\", CORPUS_PATH)\n",
    "print(\"Chars:\", len(corpus_text))\n",
    "print(\"Tokens (whitespace):\", len(corpus_text.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cbbd6",
   "metadata": {},
   "source": [
    "Task 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b0973",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b46a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 326052\n",
      "First 40 tokens: ['M', 'r', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved']\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(text: str):\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().split(\" \")\n",
    "\n",
    "text = CORPUS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "tokens = simple_tokenize(text)\n",
    "\n",
    "print(\"Num tokens:\", len(tokens))\n",
    "print(\"First 40 tokens:\", tokens[:40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207198de",
   "metadata": {},
   "source": [
    "Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f793998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8705\n",
      "Most common: [(',', 20667), ('.', 17591), ('the', 11793), ('to', 6467), ('and', 6304), ('a', 5302), ('of', 4923), ('”', 4571), ('Harry', 4467), ('was', 4109)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "def build_vocab(tokens, min_freq=2):\n",
    "    counts = Counter(tokens)\n",
    "    itos = [PAD, UNK]\n",
    "    for tok, c in counts.most_common():\n",
    "        if c >= min_freq and tok not in (PAD, UNK):\n",
    "            itos.append(tok)\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "    return stoi, itos, counts\n",
    "\n",
    "min_freq = 2\n",
    "stoi, itos, counts = build_vocab(tokens, min_freq=min_freq)\n",
    "pad_idx = stoi[PAD]\n",
    "unk_idx = stoi[UNK]\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Most common:\", counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596364f",
   "metadata": {},
   "source": [
    "Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de804ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 ids: [2540, 1, 3, 6, 222, 3, 599, 2, 8, 904, 625, 2, 1055, 1056, 2, 39, 2115, 5, 194, 23, 35, 39, 1556, 1319, 2, 2022, 19, 73, 196, 3]\n"
     ]
    }
   ],
   "source": [
    "def numericalize(tokens, stoi, unk_idx):\n",
    "    return [stoi.get(tok, unk_idx) for tok in tokens]\n",
    "\n",
    "ids = numericalize(tokens, stoi, unk_idx)\n",
    "print(\"First 30 ids:\", ids[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9725d",
   "metadata": {},
   "source": [
    "Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e2acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([8151, 40]) Y: torch.Size([8151, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 40\n",
    "\n",
    "def make_sequences(ids, seq_len):\n",
    "    n = (len(ids) - 1) // seq_len * seq_len\n",
    "    x = torch.tensor(ids[:n], dtype=torch.long)\n",
    "    y = torch.tensor(ids[1:n+1], dtype=torch.long)\n",
    "    x = x.view(-1, seq_len)\n",
    "    y = y.view(-1, seq_len)\n",
    "    return x, y\n",
    "\n",
    "X, Y = make_sequences(ids, seq_len)\n",
    "print(\"X:\", X.shape, \"Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5000894",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb94ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 115\n",
      "Val batches: 13\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def make_loaders(X, Y, batch_size=64, train_ratio=0.9):\n",
    "    n = X.size(0)\n",
    "    perm = torch.randperm(n)\n",
    "    X, Y = X[perm], Y[perm]\n",
    "    n_train = int(n * train_ratio)\n",
    "    X_train, Y_train = X[:n_train], Y[:n_train]\n",
    "    X_val, Y_val = X[n_train:], Y[n_train:]\n",
    "    train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, Y_val), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = make_loaders(X, Y, batch_size=64)\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e5ea2",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim  = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout    = 0.4\n",
    "\n",
    "learning_rate = 1e-3\n",
    "num_epochs    = 45\n",
    "grad_clip     = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef6fb9",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf335cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: [B, T] integer token ids\n",
    "        returns:\n",
    "          logits: [B, T, V]\n",
    "          hidden: (h_n, c_n)\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.lstm(emb, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611f615",
   "metadata": {},
   "source": [
    "Initialize model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8276a8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModelLSTM(\n",
       "  (embedding): Embedding(8705, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=8705, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModelLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    pad_idx=pad_idx\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69558441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, data_loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for xb, yb in data_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits, _ = model(xb)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            yb.view(-1)\n",
    "        )\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.numel()\n",
    "        total_tokens += xb.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
    "    return avg_loss, ppl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955826c0",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd79b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 6.7126, PPL: 822.67 | Val Loss: 6.1423, PPL: 465.14\n",
      "Epoch 02 | Train Loss: 5.8888, PPL: 360.96 | Val Loss: 5.7221, PPL: 305.56\n",
      "Epoch 03 | Train Loss: 5.5252, PPL: 250.93 | Val Loss: 5.4634, PPL: 235.90\n",
      "Epoch 04 | Train Loss: 5.2760, PPL: 195.59 | Val Loss: 5.2901, PPL: 198.36\n",
      "Epoch 05 | Train Loss: 5.0969, PPL: 163.52 | Val Loss: 5.1707, PPL: 176.04\n",
      "Epoch 06 | Train Loss: 4.9580, PPL: 142.31 | Val Loss: 5.0846, PPL: 161.51\n",
      "Epoch 07 | Train Loss: 4.8441, PPL: 126.99 | Val Loss: 5.0177, PPL: 151.07\n",
      "Epoch 08 | Train Loss: 4.7455, PPL: 115.06 | Val Loss: 4.9619, PPL: 142.86\n",
      "Epoch 09 | Train Loss: 4.6587, PPL: 105.50 | Val Loss: 4.9178, PPL: 136.71\n",
      "Epoch 10 | Train Loss: 4.5812, PPL: 97.63 | Val Loss: 4.8824, PPL: 131.95\n",
      "Epoch 11 | Train Loss: 4.5098, PPL: 90.90 | Val Loss: 4.8542, PPL: 128.28\n",
      "Epoch 12 | Train Loss: 4.4442, PPL: 85.13 | Val Loss: 4.8261, PPL: 124.73\n",
      "Epoch 13 | Train Loss: 4.3835, PPL: 80.12 | Val Loss: 4.8059, PPL: 122.23\n",
      "Epoch 14 | Train Loss: 4.3256, PPL: 75.61 | Val Loss: 4.7877, PPL: 120.02\n",
      "Epoch 15 | Train Loss: 4.2709, PPL: 71.59 | Val Loss: 4.7763, PPL: 118.66\n",
      "Epoch 16 | Train Loss: 4.2195, PPL: 68.00 | Val Loss: 4.7626, PPL: 117.04\n",
      "Epoch 17 | Train Loss: 4.1702, PPL: 64.73 | Val Loss: 4.7579, PPL: 116.50\n",
      "Epoch 18 | Train Loss: 4.1224, PPL: 61.71 | Val Loss: 4.7489, PPL: 115.45\n",
      "Epoch 19 | Train Loss: 4.0765, PPL: 58.94 | Val Loss: 4.7462, PPL: 115.14\n",
      "Epoch 20 | Train Loss: 4.0322, PPL: 56.38 | Val Loss: 4.7437, PPL: 114.86\n",
      "Epoch 21 | Train Loss: 3.9885, PPL: 53.97 | Val Loss: 4.7450, PPL: 115.01\n",
      "Epoch 22 | Train Loss: 3.9453, PPL: 51.69 | Val Loss: 4.7433, PPL: 114.81\n",
      "Epoch 23 | Train Loss: 3.9034, PPL: 49.57 | Val Loss: 4.7451, PPL: 115.02\n",
      "Epoch 24 | Train Loss: 3.8631, PPL: 47.61 | Val Loss: 4.7472, PPL: 115.26\n",
      "Epoch 25 | Train Loss: 3.8233, PPL: 45.75 | Val Loss: 4.7527, PPL: 115.89\n",
      "Epoch 26 | Train Loss: 3.7845, PPL: 44.01 | Val Loss: 4.7587, PPL: 116.59\n",
      "Epoch 27 | Train Loss: 3.7461, PPL: 42.35 | Val Loss: 4.7651, PPL: 117.34\n",
      "Epoch 28 | Train Loss: 3.7074, PPL: 40.75 | Val Loss: 4.7727, PPL: 118.24\n",
      "Epoch 29 | Train Loss: 3.6700, PPL: 39.25 | Val Loss: 4.7790, PPL: 118.98\n",
      "Epoch 30 | Train Loss: 3.6330, PPL: 37.83 | Val Loss: 4.7896, PPL: 120.25\n",
      "Epoch 31 | Train Loss: 3.5969, PPL: 36.49 | Val Loss: 4.7982, PPL: 121.29\n",
      "Epoch 32 | Train Loss: 3.5598, PPL: 35.16 | Val Loss: 4.8100, PPL: 122.73\n",
      "Epoch 33 | Train Loss: 3.5250, PPL: 33.95 | Val Loss: 4.8198, PPL: 123.94\n",
      "Epoch 34 | Train Loss: 3.4899, PPL: 32.78 | Val Loss: 4.8321, PPL: 125.47\n",
      "Epoch 35 | Train Loss: 3.4549, PPL: 31.65 | Val Loss: 4.8460, PPL: 127.24\n",
      "Epoch 36 | Train Loss: 3.4208, PPL: 30.59 | Val Loss: 4.8555, PPL: 128.44\n",
      "Epoch 37 | Train Loss: 3.3872, PPL: 29.58 | Val Loss: 4.8687, PPL: 130.16\n",
      "Epoch 38 | Train Loss: 3.3530, PPL: 28.59 | Val Loss: 4.8837, PPL: 132.12\n",
      "Epoch 39 | Train Loss: 3.3196, PPL: 27.65 | Val Loss: 4.8987, PPL: 134.12\n",
      "Epoch 40 | Train Loss: 3.2857, PPL: 26.73 | Val Loss: 4.9150, PPL: 136.31\n",
      "Epoch 41 | Train Loss: 3.2523, PPL: 25.85 | Val Loss: 4.9292, PPL: 138.27\n",
      "Epoch 42 | Train Loss: 3.2199, PPL: 25.03 | Val Loss: 4.9447, PPL: 140.43\n",
      "Epoch 43 | Train Loss: 3.1886, PPL: 24.25 | Val Loss: 4.9634, PPL: 143.08\n",
      "Epoch 44 | Train Loss: 3.1560, PPL: 23.48 | Val Loss: 4.9823, PPL: 145.81\n",
      "Epoch 45 | Train Loss: 3.1236, PPL: 22.73 | Val Loss: 4.9961, PPL: 147.83\n",
      "Best epoch: 22, best val loss: 4.7433\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_ppl = run_epoch(model, train_loader, train=True)\n",
    "    val_loss, val_ppl = run_epoch(model, val_loader, train=False)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, PPL: {train_ppl:.2f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, PPL: {val_ppl:.2f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:# save best model based on validation loss\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"stoi\": stoi,\n",
    "                \"itos\": itos,\n",
    "                \"pad_idx\": pad_idx,\n",
    "                \"unk_idx\": unk_idx,\n",
    "                \"vocab_size\": vocab_size,\n",
    "                \"embed_dim\": embed_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"num_layers\": num_layers,\n",
    "                \"dropout\": dropout,\n",
    "                \"seq_len\": seq_len\n",
    "            },\n",
    "            \"artifacts/lm_lstm_best.pt\"\n",
    "        )\n",
    "\n",
    "print(f\"Best epoch: {best_epoch}, best val loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ebe01",
   "metadata": {},
   "source": [
    "Validation loss decreased steadily during the early training phase and reached its minimum at epoch 22. After this point, training loss continued to decrease while validation loss began to increase, indicating the onset of overfitting. Therefore, the final model was selected based on the lowest validation loss using an early stopping criterion rather than the last training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04057cb",
   "metadata": {},
   "source": [
    "Task 3. Text Generation - Web Application Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model ✅ device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckpt = torch.load(\"artifacts/lm_lstm_best.pt\", map_location=device)\n",
    "\n",
    "stoi = ckpt[\"stoi\"]\n",
    "itos = ckpt[\"itos\"]\n",
    "pad_idx = ckpt[\"pad_idx\"]\n",
    "unk_idx = ckpt[\"unk_idx\"]\n",
    "vocab_size = ckpt[\"vocab_size\"]\n",
    "\n",
    "embed_dim  = ckpt[\"embed_dim\"]\n",
    "hidden_dim = ckpt[\"hidden_dim\"]\n",
    "num_layers = ckpt[\"num_layers\"]\n",
    "dropout    = ckpt[\"dropout\"]\n",
    "seq_len    = ckpt[\"seq_len\"]\n",
    "\n",
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.embedding(x)\n",
    "        out, hidden = self.lstm(emb, hidden) \n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "\n",
    "model = LanguageModelLSTM(vocab_size, embed_dim, hidden_dim, num_layers, dropout, pad_idx).to(device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model ✅\", \"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19225924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str):\n",
    "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().split(\" \") if text.strip() else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52911b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filter(logits: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    if k is None or k <= 0 or k >= logits.numel():\n",
    "        return logits\n",
    "    values, idx = torch.topk(logits, k)\n",
    "    filtered = torch.full_like(logits, float(\"-inf\"))\n",
    "    filtered[idx] = logits[idx]\n",
    "    return filtered\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(prompt: str, max_new_tokens: int = 60, temperature: float = 1.0, top_k: int = 50) -> str:\n",
    "    model.eval()\n",
    "\n",
    "    prompt_tokens = simple_tokenize(prompt)\n",
    "    prompt_ids = [stoi.get(t, unk_idx) for t in prompt_tokens]\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [unk_idx]\n",
    "\n",
    "    x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    logits, hidden = model(x, None)\n",
    "\n",
    "    generated = list(prompt_ids)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_logits = logits[0, -1]\n",
    "        next_logits = next_logits / max(temperature, 1e-6)\n",
    "        next_logits = top_k_filter(next_logits, top_k)\n",
    "\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        generated.append(next_id)\n",
    "\n",
    "        x_next = torch.tensor([[next_id]], dtype=torch.long, device=device)\n",
    "        logits, hidden = model(x_next, hidden)\n",
    "\n",
    "    return \" \".join(itos[i] if i < len(itos) else \"<unk>\" for i in generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7b4be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter is . We’re going to be getting into , while I’d find out of all this . They told him what the most is at Hogwarts School of wizards <unk> you coming . …” Dumbledore came to Harry , who was holding a Slytherin table before he’d gone over , and he had never heard a moment , the Snitch he\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Harry Potter is\", max_new_tokens=60, temperature=1.0, top_k=50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
